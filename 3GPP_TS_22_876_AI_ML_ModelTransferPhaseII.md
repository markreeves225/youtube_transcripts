[Youtube Video Link](https://www.youtube.com/watch?v=xkkZZ9f3W9g)



---

Welcome to this presentation on the 3GPP TR22 876 version 19.10, which focuses on the study of AI/ML model transfer Phase 2 for Release 19. This study focuses on the application and optimization of AI/ML within the 5G system, specifically looking at the transfer of AI/ML models between different entities in the network. I'd like to point out quickly here how Phase 1 is different from Phase 2 and what are the usual various phases of these kinds of studies. Phase 1 study usually focuses on initial exploration, use cases, basic requirements, and gap analysis. Phase 1 outcomes include feasibility confirmation and initial recommendations. Phase 2 study, just like the document here, focuses on advanced exploration, detailed use cases, enhanced requirements, comprehensive gap analysis, proposed solutions, and performance indicators (KPIs). The outcomes for Phase 2 usually include advanced recommendations and detailed technical solutions.

As of my knowledge, the cut-off date for this topic was back in October of 2021. There is no official documentation or announcement of Phase 3 or Phase 4 studies on AI/ML model transfer within the 3GPP framework. However, based on the progression from Phase 1 to Phase 2, potential future phases could be forthcoming.

Over the next 20 minutes, we will delve into various aspects of this study, exploring its relevance to modern AI/ML applications and the enhancements it brings to the 5G ecosystem. This 3GPP specification encompasses a rich tapestry of key topics and concepts. I've distilled its essence into a concise yet hopefully comprehensive presentation featuring 11 curated agenda items. These are explored across 23 crafted slides which incorporated the majority of the specification's original diagrams.

Let me go through the agenda items here quickly. First is Scope and Objectives, AI/ML Operations Overview, Proximity-Based Work Task Offloading, Local AI/ML Model Split on Factory Robots, AI/ML Model Transfer Management, 5GS (5G System) Assisted Transfer Learning, Direct Device Connection Assisted Federated Learning, Asynchronous Federated Learning, Distributed Joint Interface for 3D Object Detection, Consolidated Potential Requirements and KPIs, and lastly the Conclusion and Recommendations slide.

Let's begin. The scope of this technical report is to study the use cases and identify the functional and performance requirements necessary for efficient AI/ML operations using direct device connections. We’ll explore further on this concept here. Our study focuses on three main areas: distributed AI inference, distributed decentralized model training, and conducting a gap analysis of the existing 5G system mechanisms to identify areas needing enhancement. The ultimate goal is to ensure that the 5G system can support these AI/ML operations effectively, enabling applications such as autonomous driving, robot remote control, and video recognition.

In the initial phase, we identified three primary types of AI/ML operations: splitting operations between AI/ML endpoints, distributing and sharing AI/ML models and data over the 5G system, and utilizing distributed or federated learning. Phase 2 of this study further explores how direct device connections can enhance each of these operations, providing more efficient and effective AI/ML processes. By leveraging direct device connections, we aim to improve the communication capacity and coverage, leading to better task offloading and more efficient AI/ML training and inference.

Okay, the first big topic here is proximity-based work task offloading. What is it? As the name implies, it’s about offloading work from one device to another. Proximity-based work task offloading allows a device with low computation capacity to offload tasks to a nearby device leveraging direct device connections. This approach helps manage computation loads and improve performance without necessarily increasing data rates.

The conditions for this use case include the device using an AI model like AlexNet for image recognition and having predefined alternative splitting points during the service flow. The task offloading process involves the device discovering a nearby device, establishing a direct connection with this nearby device, and transferring intermediate data. This ensures reduced computation load on the original device while maintaining QoS parameters. Existing features support direct network connection modes, but we need new requirements for proximity-based work task offloading to complete, and then implementation comes afterward. We don’t have all the requirements as of yet, but I’m sure people are working on it or have worked on it, but made their solution proprietary.

Okay, let’s take a look at this diagram showing the AlexNet model for image recognition. This diagram, I’m going to go into depth a little bit, so stay with me. This diagram illustrates the layer-level computation and communication resource evaluation for an AlexNet model used in image recognition. You can see the mobile device on the left, the little icon there, the mobile device that’s performing the initial computation for image recognition using the AlexNet model. Look to the right, the right side you can see the network server, the server that takes over part of the computational tasks from the mobile device to reduce the computation load on the mobile device.

Then take a look at the candidate split points. There are four of them on the top of the histogram. These are specific points in the neural network where the computation can be split between the mobile device and the network server. Now look at the Y axis on the left, which indicates the layer latency, the latency associated with computing each layer of the AlexNet model on the mobile device. The Y axis on the right indicates the size of the output data, the size of the data outputted by each layer, which needs to be transmitted if the computation is offloaded to the network server. Now, the X axis on the bottom represents the different layers in the AlexNet model, including convolutional layers (CNV), pooling layers (POOL), and fully connected layers (FC). You can see those acronyms written sideways near the bottom there on this histogram and then the final layer called the softmax layer.

So, the X axis includes all of these layers. Now the split points, the candidate split points starting from the left to the right represent the initial input to the neural network. At split point zero, there is no latency at this point as no computation has been performed yet. The size of the input data is relatively small. Next, the candidate split point one after POOL1 layer. This is after the first pooling layer. The layers up to this point (CONV1 and POOL1) show significant latency indicating substantial computational effort. The size of the input data from POOL1 is larger compared to the input reflecting the intermediate data generated after initial layers of processing.

Candidate split point two after POOL2 layer, this is after the second pooling layer. The cumulative latency up to this point includes the latency from CONV2 and POOL2, which is still substantial but less than the first split point. The data size here is smaller than at split point one, indicating some reduction due to pooling operations. Candidate split point three after POOL5 layer, this is after the fifth pooling layer. The cumulative latency up to POOL5 is lower than the previous split points reflecting reduced computational demand as the network progresses. The data size is significantly reduced making it more feasible to transmit intermediate data if offloading is necessary.

Now the final split point, candidate split point four, final output. This is at the network’s final output stage. The cumulative latency includes all previous layers and the fully connected layers. The size of the final output data is minimal as it represents the condensed classification result. So for computational load management, we need to select appropriate split points that manage the computational load on the mobile device effectively. For example, if the device has low computation capacity or battery, earlier split points can be chosen to offload more computation to the network server. The size of the output data at each split point influences the data transmission requirements. Later split points result in smaller data sizes reducing the transmission load on the network. We also need to consider latency. The latency associated with each layer impacts the decision on where to split the computation. Lower latency layers can be processed on the mobile device while high latency layers can be offloaded to reduce the overall processing time.

Okay, that’s all I’d like to say for this diagram. I think it should be quite self-evident what it is trying to convey. Moving on to the next diagram, this is also about AlexNet, looking at it from a slightly different angle, not a histogram now. This is really sort of a flow, how things flow. This diagram illustrates two scenarios related to task offloading for AI/ML inference using, again, an AlexNet model for image recognition.

The two scenarios are: if you look at the bottom there, scenario A is called No Task Offloading. As the name implies, the mobile device (UEA) in this instance performs all computations up to a certain layer and sends intermediate data to the application server. Look at the top there, the application server on the top of this picture, sending the intermediate data to the application server for further processing. The scenario B, Task Offloading by UEB, describes the mobile device (UEA) offloading part of the computation to a nearby device (UEB) using a direct device connection (synonymous to sidelink, which you may have heard of). Sidelink is essentially device-to-device direct device connection, which then completes the computation and sends intermediate data to the application server.

In scenario A, no task offloading, the mobile device (UEA) performs the computations for layers 1 to 15 of the AlexNet model. After processing layers 1 to 15, UEA generates intermediate data, which is 0.02 megabytes per frame. It’s written very small on the picture, but I think you may be able to see it, it’s still pretty clear, 0.02 megabytes per frame in scenario A. The intermediate data is transmitted to the application server via the NG-RAN (next generation radio access network) using the UU interface (I’m sure

 you’re familiar with it), UU interface from the mobile device to the NG-RAN, this connection. In scenario B, task offloading by UEB, UEA performs computations for layers 1 to 5 of the AlexNet model, the first five layers. After processing layers 1 to 5, UEA generates intermediate data, which is 1.5 megabytes per frame. This intermediate data is transmitted to UEB, the offloading device. UEB then performs the computations for layers 6 to 15 using the received intermediate data. After processing layers 6 to 15, UEB generates intermediate data which is again 0.02 megabytes per frame. This intermediate data is transmitted to the application server via the NG-RAN using the UU interface from UEB to the NG-RAN.

Moving on to the third diagram. This one illustrates the architecture for local AI/ML model split in factory robots, utilizing direct device connections. The architecture involves a mobile robot connected to an application server via NG-RAN (which you can see the boxes on the bottom there with the NG-RAN label). The mobile robot comprises two components, each handling different aspects of the AI/ML model split. We have the robot controller (UEC) and the robot arm (UED). The robot controller is responsible for initial AI/ML model computations up to a specified split point, which are then offloaded to the robot arm via a direct device connection for further processing. The direct device connection allows for seamless intermediate data transfer between the controller and the arm, enabling efficient task offloading.

This one is a little bit more straightforward in my opinion. The architecture illustrates the use of both local AI/ML model split and proximity-based task offloading in an industrial setting. We can see the NG-RAN network there, the dotted arrowed lines representing UU connections from UEC and UED to the NG-RAN (the UU connection). Then you can see the application server on top of the picture, there is also a database, which I think is there to illustrate a higher layer service from the application server (to the application server to retrieve data from the database). The direct device connection between UEC and UED is enabled by sidelink, providing low-latency communication.

Now we move on to the next one, which I think may be the most important diagram in the specification, this is the study of different scenarios. The next diagram illustrates the architecture for managing AI/ML model transfer across the 5G system. This architecture encompasses the application server, which plays a key role in model management and distribution to various UE devices. The focus here is on two main types of model transfers. First, model dissemination, where the application server distributes AI/ML models to multiple UEs. Second, model update and reassembly, involving incremental updates to AI/ML models deployed in UEs. This enables efficient handling of model training and inference across the network, optimizing performance and resource utilization.

The architecture illustrates the comprehensive framework for AI/ML model transfer management. Starting from the top, we have the application server on the left of this picture. It orchestrates model management, the top box there, you see the box for model management. This includes model generation, distribution, and updates. You can see the solid arrows representing model dissemination. The application server sends a model to the model repository (sort of like a database). Then, the application server distributes the AI/ML model to multiple UEs in the network. In the bottom right of the picture, the solid arrows represent model dissemination to UEs. This process enables devices to download and utilize the latest AI/ML models for their respective tasks.

Now, let’s look at the dashed arrows representing model updates and reassembly. When incremental updates to the AI/ML model are needed, the application server generates updates, which are then sent to the UEs. The UEs then reassemble the full updated model using the incremental updates. This method ensures that devices always have the most recent version of the model without needing to download the entire model from scratch each time.

Next, let’s look at another architecture. This one shows the architecture for supporting 5GS-assisted transfer learning across multiple UEs. The architecture focuses on how transfer learning techniques can be leveraged to enhance AI/ML model training and inference using assistance from the 5G system. This involves an application server coordinating transfer learning operations, distributing initial models, collecting intermediate results, and refining the models based on aggregated data from multiple UEs.

Let’s look at the components first. We have the application server on the top of the picture (left-hand side), model repository (this can be a database of models used to store and manage AI/ML models), and multiple UEs. These are the devices that will participate in transfer learning. The process involves multiple steps, starting from the application server generating an initial model, which is then stored in the model repository. The application server sends the initial model to various UEs, enabling them to perform local training using their unique data. After the UEs complete their local training, they send their intermediate results back to the application server.

Next, the application server aggregates the intermediate results from the UEs, refines the AI/ML model, and generates an updated model. This updated model is then redistributed to the UEs for further local training. This iterative process continues, gradually improving the model's accuracy and performance. Through 5GS assistance, the transfer learning process can be optimized by efficiently managing data transfer, minimizing latency, and ensuring reliable communication between the application server and the UEs. This architecture highlights the potential of using transfer learning to enhance AI/ML model training and inference in a distributed and collaborative manner, leveraging the strengths of the 5G system.

Now the next diagram shows the architecture for enabling federated learning using direct device connections. This architecture focuses on leveraging direct device connections to enhance federated learning operations, allowing multiple devices to collaboratively train a shared AI/ML model while keeping their data local.

We have multiple UEs that participate in federated learning. Each UE locally trains the shared AI/ML model using its own data, which remains on the device. The application server (located on the top left corner there) coordinates the federated learning process. The key components involved are direct device connections, which provide low-latency communication between UEs. The federated learning process involves the application server sending the initial AI/ML model to each participating UE. Each UE then performs local training, generating intermediate model updates based on its data. Instead of sending the entire dataset to the server, the UEs send their model updates to the application server using direct device connections.

The application server aggregates the updates from all participating UEs to generate a refined global model. This global model is then redistributed to the UEs for further local training. The iterative process continues, improving the model’s accuracy and performance. This architecture highlights the importance of direct device connections in federated learning. By facilitating low-latency communication, direct device connections enable efficient aggregation of model updates and reduce the time required for each training iteration. This approach enhances the overall performance of federated learning, making it feasible to train complex AI/ML models collaboratively while preserving data privacy.

The next diagram illustrates asynchronous federated learning, highlighting how different UE devices can operate independently without needing to synchronize their updates at the same time. This approach enhances the flexibility and efficiency of federated learning, especially in dynamic and heterogeneous network environments. Let’s look at the components involved.

We have multiple UEs participating in asynchronous federated learning, an application server coordinating the process. The key difference here is that each UE can perform local training and send model updates at different times. The asynchronous federated learning process involves the application server sending the initial AI/ML model to each UE, allowing them to perform local training. Unlike synchronous federated learning, where updates are aggregated at fixed intervals, in asynchronous federated learning, each UE sends its model updates to the application server as soon as local training is completed. The application server continuously receives and aggregates updates from different UEs independently.

The diagram highlights that the application server maintains a global model, which is continuously refined based on the updates received from the UEs. This approach ensures that the global model evolves dynamically as new updates arrive. Asynchronous federated learning accommodates the varying availability and computation capacities of UEs. Some UEs may complete their local training quickly, while others might take longer. By allowing updates to be sent independently, asynchronous federated learning optimizes the utilization of network resources and reduces the idle time for UEs waiting for synchronization points.

This architecture showcases the benefits of asynchronous federated learning in dynamic and heterogeneous networks. It enables continuous model improvement, minimizes idle times, and enhances the overall efficiency of federated learning operations.

Finally, the last diagram is on distributed joint inference for 3D object detection. This diagram illustrates the architecture for enabling distributed joint inference using direct device connections, specifically focusing on 3D object detection. This approach leverages multiple devices to collaboratively perform inference tasks, enhancing the accuracy and efficiency of complex AI/ML models.

The key components involved are multiple UEs equipped with AI/ML models for 3D object detection, an application server coordinating the joint inference process, and direct device connections facilitating low-latency communication between UEs. The distributed joint inference process involves each UE capturing input data, such as images or sensor readings, and performing initial inference computations. The intermediate results generated by each UE are then shared with other participating UEs using direct device connections.

The UEs collaborate by exchanging their intermediate results, enabling them to jointly refine the inference outcomes. The combined results from all UEs are then sent to the application server for final processing and decision-making. This architecture highlights the benefits of distributed joint inference in scenarios requiring high accuracy and real-time processing, such as 3D object detection. By leveraging direct device connections, UEs can collaborate efficiently, reducing the latency and computational load on individual devices.

Now that we’ve covered the main diagrams, let’s look at some consolidated potential

 gains of these architectures for efficient AI/ML model management and distributed learning. There are three key areas of impact, namely efficiency, scalability, and performance. The architectures demonstrated above optimize resource utilization by offloading tasks to different devices, reducing the burden on individual UEs. They also enable scalable AI/ML model management through coordinated transfer learning, federated learning, and joint inference. Finally, the architectures enhance the performance of AI/ML models by leveraging direct device connections, low-latency communication, and efficient aggregation of model updates. 


Here's the corrected transcript with spelling mistakes fixed:

---

5G system-assisted transfer learning for trajectory prediction between two user equipments (UEs), especially or specifically here, it is clearly stated UE1 and UE2. The process depicted involves transferring a pre-trained AI/ML model from UE1 to UE2, where it is then fine-tuned based on local data from UE2. This method leverages the 5G network to facilitate the model transfer efficiently. The device that initially uses and holds the pre-trained AI model is UE1. The device that receives the AI/ML model from UE1 and fine-tunes it based on its local data is UE2. The 5G network supports and facilitates the model transfer processes between UE1 and UE2. It’s pretty clearly stated here, so UE1 has an AI model that has been pre-trained and used for a specific task such as trajectory prediction in intelligent driving, which is one use case of many use cases. Its functionality is using this model, which has been pre-trained on data relevant to the environment and use cases specific to UE1. Then, this model in UE1 is transferred from UE1 to UE2 through the 5G network. The transfer allows UE2 to leverage the pre-trained model from UE1, which can significantly reduce the training time and computational resources needed for UE2 to achieve a similar level of performance. The beauty of this is, and I said this already, that after the model is transferred to UE2, it can then adjust the model based on its local data, reflecting the environment unique to UE2. This fine-tuning based on local data is very beneficial after the model transfer. What fine-tuning does is simply involve adjusting the model parameters to better fit the specific environment and use cases of UE2. The outcome would be that the fine-tuned model is optimized for the conditions and requirements of UE2, providing or improving its performance and accuracy. So, this is a simple enough diagram.

Next is assisted Federated Learning. Federated learning, which is a slightly different concept, still is rather simple. As the name implies, you do learning collaboratively with direct device connections, allowing devices to share training results locally, not training models. This is sharing training results locally, improving model accuracy and reducing reliance on a central server. Preconditions include an application server for Federated Learning and transmission delay requirements. The service flows detail the decentralized averaging method, where local model updates are aggregated globally. Post-conditions ensure continuous model training and optimized Federated learning performance. We have a picture full of this to explain further. Existing features support monitoring network resource utilization and predicting network condition changes. Again, we need good written requirements if we want to enhance Federated learning through direct device connections. That is yet to be worked on, and people are publishing what their specific requirements are based on use cases. Here is the diagram that will explain further the concept of Federated Learning (FL) in short with a decentralized averaging method, specifically using the Lazy Metropolis algorithm. Lazy Metropolis algorithm, if you want to look at the details, Google it. As I always say, all the information is at your fingertips if you want to learn more about any of these. You can just look it up on the internet. This Lazy Metropolis algorithm is one of the two methods. The diagram here compares the performance of the original Federated Learning with Federated Learning using this Lazy Metropolis method in terms of identification accuracy over the number of learning steps or learning iterations. The decentralized averaging method aims to enhance the performance of Federated learning by reducing communication overhead and enabling more devices to participate in the learning process. Here we have on the diagram device A and device B. These devices participate in the FL process. I'm going to say FL process meaning Federated learning process. So, device A and device B participate in the FL process using the Lazy Metropolis algorithm. The communication paths represent, and we have two of them here, these communication paths represent the data transmission between devices and the central server, which is the original FL, and then between devices themselves, which is using the Lazy Metropolis FL. Those are the two. The distances between devices are indicated, showing the range of communication. So, we are looking at a performance graph. This is a performance graph. Essentially, the Y-axis represents identification accuracy, the accuracy of the model in identifying or classifying data. The X-axis represents the number of steps or iterations in the learning process. The red dashed line indicates the original FL, the performance of the traditional FL method. The black solid line represents the performance of the Federated learning method using the Lazy Metropolis algorithm. We are comparing the two, the original and the FL using the Lazy Metropolis algorithm, and that is represented by the black solid line. So, take a look at the two communication paths, right? The original FL is green in color near the top there. In the original FL, devices transmit their local model updates directly to the central server. The central server aggregates these updates and sends back the global model. This method relies heavily on the central server and requires all devices to communicate with it, which can lead to high communication overhead and delays, especially for devices with poor connectivity. The other path, purple in color, you can see the FL with Lazy Metropolis is the purple path. The purple path indicates devices communicate with each other to share their local model updates. The Lazy Metropolis algorithm is used to aggregate these updates in a decentralized manner. This method reduces reliance on the central server, or gets rid of it altogether. It doesn't go to the central server and allows devices to update their models more efficiently by sharing updates with nearby devices. The communication paths and distances between devices show the decentralized nature of this approach, enabling more flexibility and reducing the communication burden on the central server. There are the two different lines, as we talked about earlier, the red dashed line and the black solid line. The red dashed line, which indicates the identification accuracy of the original FL method, increases steadily with the number of learning steps but plateaus at around 0.85. You can see it plateaus around 0.85. The performance is limited by the communication overhead and the need for all devices to synchronize with the central server. That's pretty taxing. Now, the black solid line, that's FL with Lazy Metropolis. The identification accuracy of the FL with Lazy Metropolis method increases more rapidly and reaches high accuracy levels, plateauing at around 0.89. The difference is not that crazy, but if you look at the inset graph, it highlights the difference in performance between the two methods, showing that the Lazy Metropolis method consistently outperforms the original FL method in terms of identification accuracy. We know the reason why, right? We just talked about the reason why because of the overhead. This improvement is due to the decentralized averaging method, which allows for more efficient communication and aggregation of model updates, enabling more devices to participate in the learning process. There are many use cases. Image recognition is one use case that is used throughout this specification, but you can think of all the others. I'm not going into more use cases. If you want more, for reference, you can look up the spec, but the concept is exactly the same using what we just described. That's the idea of assisted Federated learning.

Moving on, this is an even simpler diagram, but we'll take a quick look at it. This diagram illustrates a scenario where Bob and Alice perform decentralized Federated learning (FL) using direct device communication, bypassing the central application server. You can see the two big red crosses here indicating the communication between Alice and Bob are between each other and not bothering to go up to the application server. This approach is particularly useful when the UEs have poor network connectivity to the central server but can communicate directly with each other. So that's what it is. I'm not going to talk more because we already know what the application server does and what the two are doing, how they transfer models. It’s a very simple picture.

Next, let's talk about asynchronous Federated learning versus synchronous. This is asynchronous Federated learning (ASFL), as is the acronym. It enables UEs to report results when ready. When it's good and ready, it will just report the results without waiting for all devices to synchronize. This method is more resilient to varying network conditions and device capabilities. Preconditions include the availability of direct and indirect network connections and relaxed communication requirements. The service flows describe the ASFL process, where relay UEs determine QoS for member UEs. So, there's a new entity here, relay UE, doing something, determining QoS for member UEs. We’ll take a look at a picture later to further understand. Post-conditions ensure efficient model training and proper charging for indirect communication. Existing features support aggregator QoS (Quality of Service) and exposing information to third parties. Again, we need new requirements to be established if we want to fully support ASFL via direct device connections. Here’s the picture. Another simple picture, if you look at it carefully, illustrates the concept of ASFL within a group-based setup. It shows how member UEs can perform Federated learning with the help of a relay UE. Look at the middle of this picture. There's a relay UE pointing to UE1, which acts as the relay UEI. This picture shows how member UEs can perform Federated learning with the help of a relay UE, especially in scenarios where some UEs are in areas with bad network coverage. The diagram highlights the communication flow between the parameter server, relay UE, and member UEs. The parameter server is the central entity responsible for aggregating the model updates from member UEs and distributing the global model back to them. The relay UE is a special UE, UE1 here, and it’s designated to assist other UEs that have poor network connectivity. It acts as an intermediary to relay data between the parameter server and other UEs. The UEs participating in the Federated learning process are called the member UEs.

 Some member UEs have good network coverage, while others have bad network coverage. In this diagram, there are three member UEs, two with good network coverage and one with bad network coverage. The two arrows from the parameter server to the relay UE and member UEs with good network coverage represent the direct communication path for model updates. These UEs can communicate directly with the parameter server to send their local model updates and receive the global model. The arrows from the relay UE to the member UE with bad network coverage represent the indirect communication path for model updates. The relay UE assists the member UE with bad network coverage by relaying its model updates to the parameter server and delivering the global model back to it. Now, let's look at the service flow diagram. This is a more detailed view of the service flow in ASFL with relay UE involvement. It describes the steps and interactions between the parameter server, relay UE, and member UEs during the Federated learning process. The steps involved in the service flow include, first, initialization: The parameter server initializes the global model and distributes it to all member UEs and the relay UE. Second is local training: Each member UE performs local training on its own data using the received global model. Third, asynchronous reporting: Member UEs send their local model updates to the parameter server. UEs with good network coverage send updates directly, while the UE with bad network coverage sends its updates via the relay UE. The next step is aggregation: The parameter server aggregates the local model updates from all member UEs to update the global model. Then, the next step is global model distribution: The parameter server distributes the updated global model to all member UEs and the relay UE. In cases where some UEs cannot be reached directly, the relay UE assists by relaying the global model to those UEs. The final step is iteration: The process iterates, with member UEs performing local training on the updated global model and reporting their updates asynchronously. The relay UE continues to assist UEs with bad network coverage throughout the iterations. This flow ensures that the Federated learning process can continue efficiently even when some UEs have poor network connectivity, leveraging the relay UE to maintain communication with the parameter server. So, that's the idea of asynchronous Federated learning. We will not talk more about other things because it’s self-explanatory. We’re ready to move on to the next.

Moving on, another small diagram. This diagram illustrates hierarchical Federated learning, so HFL for short. The hierarchy is evident here with different layers. For example, it starts at the lowest level with the federated edge nodes, also known as FEN, or federated edge nodes, with connections between them showing communication paths. Edge nodes, which are not represented here in this picture, but edge nodes are the bottom layer of the hierarchy. Federated edge nodes are the middle layer of the hierarchy, and the application server is the top layer of the hierarchy. Each level in the hierarchy has its own Federated learning process, with models being aggregated and updated at each level before being passed up to the next level. This hierarchical approach allows for more efficient aggregation and distribution of models, especially in large-scale networks with many devices. The communication paths between nodes at each level show how local models are aggregated and updated before being passed to the next level. At the federated edge nodes level, local models from multiple edge nodes are aggregated to form a regional model. This regional model is then sent to the application server, where it is further aggregated to form the global model. The global model is then distributed back down the hierarchy, with updates being passed from the application server to federated edge nodes and then to edge nodes. This hierarchical approach allows for more efficient and scalable Federated learning by reducing the communication overhead and enabling more efficient aggregation of models at different levels of the network. So that's hierarchical Federated learning. It’s very self-explanatory. If you want to read more, you can go ahead and look up the spec or Google more. That's the idea of hierarchical Federated learning. Let's move on to the next one.

The final one here is Group-Based Federated learning. So, Federated learning with groups, right? The diagram here describes a scenario in which multiple groups of UEs perform Federated learning independently. So, it's pretty simple in that regard. In this diagram, UEs in Group 1 and Group 2 conduct Federated learning within their respective groups without interacting with UEs in other groups. There is no communication between UEs in different groups. Group-based Federated learning is useful when UEs within a group share similar data characteristics, and it allows for more efficient and targeted model training. The communication paths within each group show how local models are aggregated and updated within the group before being sent to the parameter server. UEs within Group 1 communicate with each other to share local model updates and aggregate them to form a group model. This group model is then sent to the parameter server, where it is aggregated with group models from other groups to form the global model. The global model is then distributed back to each group, with updates being passed down to the UEs within the group. So, it's pretty simple. Each group of UEs performs its own Federated learning process, with models being aggregated and updated within the group before being sent to the parameter server. This approach allows for more efficient and targeted model training by leveraging the similar data characteristics within each group. And so, this concludes my presentation. If you want to read more, go look at the specification. If you want to reach out to me to discuss any use cases, that is something I am open to. That's the presentation. Now, we are ready for questions. Thank you.

---

